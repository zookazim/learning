{
    "name": "c2_rmhs_nb_validate_submitted_data",
    "properties": {
        "description": "Ensure a csv file has a header row and if not insert one",
        "folder": {
            "name": "c2_rmhs"
        },
        "nbformat": 4,
        "nbformat_minor": 2,
        "bigDataPool": {
            "referenceName": "synsppool1",
            "type": "BigDataPoolReference"
        },
        "sessionProperties": {
            "driverMemory": "28g",
            "driverCores": 4,
            "executorMemory": "28g",
            "executorCores": 4,
            "numExecutors": 2,
            "runAsWorkspaceSystemIdentity": false,
            "conf": {
                "spark.dynamicAllocation.enabled": "false",
                "spark.dynamicAllocation.minExecutors": "2",
                "spark.dynamicAllocation.maxExecutors": "2",
                : "c6e5e9af-5ec5-407a-899d-f3cb346a5c62"
            }
        },
        "metadata": {
            "saveOutput": false,
            "enableDebugMode": false,
            "kernelspec": {
                "name": "synapse_pyspark",
                "display_name": "Synapse PySpark"
            },
            "language_info": {
                "name": "python"
            },
            "a365ComputeOptions": {
                "id": "/subscriptions/64b96407-6243-4e44-9932-e2d6c43504bd/resourceGroups/wah-mhcb-n-aze-rg-dlz1/providers/Microsoft.Synapse/workspaces/wah-mhcb-d-aze-syn/bigDataPools/synsppool1",
                "name": "synsppool1",
                "type": "Spark",
                "endpoint": "https://wah-mhcb-d-aze-syn.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/synsppool1",
                "auth": {
                    "type": "AAD",
                    "authResource": "https://dev.azuresynapse.net"
                },
                "sparkVersion": "3.4",
                "nodeCount": 3,
                "cores": 8,
                "memory": 56,
                "automaticScaleJobs": false
            },
            "sessionKeepAliveTimeout": 30
        },
        "cells": [
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Description\n",
                    "\n",
                    "This notebook takes an input `.csv` file as submitted by agencies and performs the following:\n",
                    "\n",
                    "1. Checks if first row is a header row\n",
                    "    - If not, grab header row from a template file and insert as header row\n",
                    "    - If is a header row, overwrites it with the template's header row\n",
                    "\n",
                    "2. Compare file with expected schema\n",
                    "    - If incorrect number of columns in the file, output an error and fail the run\n",
                    "\n",
                    "3. Check for empty `a_res_service_unit_id`, `b_date_of_referral` or `k_person_id`, together these fields form the unique identifier (PK) for the record \n",
                    "    - If any empty rows are found, output an error and fail the run\n",
                    "\n",
                    "4. Check for duplicated rows within the file.\n",
                    "    - If entire rows are duplicated, remove duplicated rows\n",
                    "    - If duplicates are violating the PK, output an error and fail the run\n",
                    "\n",
                    "This notebook while write validation errors/warnings to a stg_error_[entity].csv file in the Bronze storage container (configurable).\n",
                    "\n",
                    "## Assumptions:\n",
                    "\n",
                    "The incoming file is in a `.csv` format\n",
                    ""
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Setting up the notebook"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# Imports\n",
                    "from pyspark.sql import SparkSession\n",
                    "from pyspark.sql.types import *\n",
                    "from pyspark.sql import DataFrame\n",
                    "from pyspark.sql.functions import *\n",
                    "from pyspark.sql.functions import col\n",
                    "from pyspark.sql import functions as F\n",
                    "import pyspark.sql.functions as func\n",
                    "import pyspark.sql.functions as f\n",
                    "import pandas as pd\n",
                    "import re\n",
                    "from delta import DeltaTable\n",
                    "import os\n",
                    "from pyspark.sql import Row\n",
                    "from pyspark.sql.window import Window\n",
                    "from pyspark.sql.functions import row_number\n",
                    "from functools import reduce"
                ],
                "execution_count": 63
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    },
                    "tags": [
                        "parameters"
                    ]
                },
                "source": [
                    "# Declaring parameters\n",
                    "run_without_pipeline = False\n",
                    "\n",
                    "if run_without_pipeline:\n",
                    "\n",
                    "    entity_name = \"poc\"   #\"rmhs\" \n",
                    "\n",
                    "    file_name = \"NGO-c2_rmhs-20250507T065309370Z-Mind Australia_20250507_poc.csv\"\n",
                    "    timestamped_directory = \"transactional/c2_rmhs/20250507131950082\"\n",
                    "    target_database = 'c2_rmhs'\n",
                    "    validation_file = \"stg_error\"\n",
                    "\n",
                    "    # Raw\n",
                    "    raw_adls_url = \"https://wahmhcbdazestraw.dfs.core.windows.net\"\n",
                    "    raw_file_system_name = \"raw\"\n",
                    "\n",
                    "    # Bronze\n",
                    "    bronze_adls_url = \"https://wahmhcbdazestbronze.dfs.core.windows.net\"\n",
                    "    bronze_file_system_name = \"bronze\"\n",
                    "    \n",
                    "    # Silver\n",
                    "    silver_adls_url = \"https://wahmhcbdazestsilver.dfs.core.windows.net\"\n",
                    "    silver_file_system_name = \"silver\"\n",
                    "\n",
                    "    # Template\n",
                    "    template_adls_url = \"https://wahmhcbdazestsilver.dfs.core.windows.net\"\n",
                    "    template_file_system = \"silver\"\n",
                    "    template_file_path = \"master and reference/templates/c2_rmhs\"\n",
                    "\n",
                    "    # Schema\n",
                    "    schema_adls_url = \"https://wahmhcbdazestsilver.dfs.core.windows.net\"\n",
                    "    schema_file_system_name = \"silver\"\n",
                    "    schema_file_path_and_name = \"master and reference/schema/c2_rmhs\"\n",
                    "\n",
                    "    # Validation rules\n",
                    "    rules_adls_url = \"https://wahmhcbdazestsilver.dfs.core.windows.net\"\n",
                    "    rules_file_system_name = \"silver\"\n",
                    "    rules_file_path = \"master and reference/validation rules/c2_rmhs\"    \n",
                    "\n",
                    "    # Exception Report\n",
                    "    exception_adls_url = \"https://wahmhcbdazestbronze.dfs.core.windows.net\"\n",
                    "    exception_file_system_name = \"bronze\"\n",
                    "\n",
                    "else:\n",
                    "\n",
                    "    entity_name = \"\"\n",
                    "\n",
                    "    file_name = \"\"\n",
                    "    timestamped_directory = \"\" \n",
                    "    target_database = ''\n",
                    "    validation_file = \"\"\n",
                    "\n",
                    "    # Raw\n",
                    "    raw_adls_url = \"\"\n",
                    "    raw_file_system_name = \"\"\n",
                    "\n",
                    "    # Bronze\n",
                    "    bronze_adls_url = \"\"\n",
                    "    bronze_file_system_name = \"\"\n",
                    "    \n",
                    "    # Silver\n",
                    "    silver_adls_url = \"\"\n",
                    "    silver_file_system_name = \"\"\n",
                    "\n",
                    "    # Template\n",
                    "    template_adls_url = \"\"\n",
                    "    template_file_system = \"\"\n",
                    "    template_file_path = \"\"\n",
                    "\n",
                    "    # Schema\n",
                    "    schema_adls_url = \"\"\n",
                    "    schema_file_system_name = \"\"\n",
                    "    schema_file_path_and_name = \"\"\n",
                    "\n",
                    "    # Validation rules\n",
                    "    rules_adls_url = \"\"\n",
                    "    rules_file_system_name = \"\"\n",
                    "    rules_file_path = \"\"    \n",
                    "\n",
                    "    # Exception Report\n",
                    "    exception_adls_url = \"\"\n",
                    "    exception_file_system_name = \"\""
                ],
                "execution_count": 64
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Initiate Spark Session"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# Create Spark Session\n",
                    "spark = SparkSession.builder.getOrCreate()"
                ],
                "execution_count": 65
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "### Set variables"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "timestamp = timestamped_directory[-12:]\n",
                    "\n",
                    "# Raw\n",
                    "raw_base_url = raw_adls_url.replace(\"https://\", \"\")\n",
                    "raw_input_file_path = f\"abfss://{raw_file_system_name}@{raw_base_url}/{timestamped_directory}/{file_name}\"\n",
                    "raw_formatted_file_path = f\"abfss://{raw_file_system_name}@{raw_base_url}/{timestamped_directory}/formatted/formatted_{file_name}\"\n",
                    "raw_delta_path = f\"abfss://{raw_file_system_name}@{raw_base_url}/{timestamped_directory}/stg_dataimport_{entity_name}\"\n",
                    "\n",
                    "print(\"Raw input file path: \" + raw_input_file_path)\n",
                    "print(\"Raw formatted file path: \" + raw_formatted_file_path)\n",
                    "print(\"Raw delta path: \"+ raw_delta_path)\n",
                    "\n",
                    "# Bronze\n",
                    "bronze_target_file = 'stg_validation_' + entity_name\n",
                    "bronze_base_url = bronze_adls_url.replace(\"https://\", \"\").lower()\n",
                    "bronze_delta_path = f\"abfss://{bronze_file_system_name}@{bronze_base_url}/{timestamped_directory}/{bronze_target_file}/\"\n",
                    "validation_table = \"stg_validation_\" + entity_name + \"_\" + timestamp\n",
                    "validation_view = \"c2_rmhs_vw_staging_data_\" + entity_name + \"_\" + timestamp\n",
                    "\n",
                    "print(\"Bronze delta path: \" + bronze_delta_path)\n",
                    "print(\"Validation table: \" + validation_table)\n",
                    "print(\"Validation view: \" + validation_view)\n",
                    "\n",
                    "# Template\n",
                    "template_name = 'submission_template_' + entity_name + '.csv'\n",
                    "template_base_url = template_adls_url.replace(\"https://\", \"\")\n",
                    "template_path = f\"abfss://{template_file_system}@{template_base_url}/{template_file_path}\" + '/' + template_name\n",
                    "\n",
                    "print (\"Template path: \" + template_path)\n",
                    "\n",
                    "# Schema\n",
                    "schema_name = 'submission_schema_' + entity_name + '.json'\n",
                    "schema_directory = schema_file_path_and_name + '/' + schema_name\n",
                    "schema_base_url = schema_adls_url.replace(\"https://\", \"\")\n",
                    "schema_path = f\"abfss://{schema_file_system_name}@{schema_base_url}/{schema_directory}\"\n",
                    "\n",
                    "print(\"Schema path: \" + schema_path)\n",
                    "print(\"Schema directory: \" + schema_directory) \n",
                    "\n",
                    "# Exception Report\n",
                    "exception_base_url = exception_adls_url.replace(\"https://\", \"\")\n",
                    "exception_folder = f\"abfss://{exception_file_system_name}@{exception_base_url}/{timestamped_directory}/\"\n",
                    "exception_file_path = exception_folder + f\"{validation_file}_{entity_name}.csv\"\n",
                    "\n",
                    "# Occupancy data\n",
                    "occupancy_delta_path = f\"abfss://{bronze_file_system_name}@{bronze_base_url}/{timestamped_directory}/stg_occupancy\"\n",
                    "\n",
                    "print(\"Exception report path: \" + exception_file_path)\n",
                    "\n",
                    "# Validation Rules\n",
                    "rules_entity_path = rules_file_path + \"/config_validation_\" + entity_name + \".json\"\n",
                    "rules_base_url = rules_adls_url.replace(\"https://\", \"\").lower()\n",
                    "rules_delta_path = f\"abfss://{rules_file_system_name}@{rules_base_url}/{rules_entity_path}\"\n",
                    "\n",
                    "print(rules_delta_path)\n",
                    "\n",
                    "# empty columns to be excluded for rmhs data submissions\n",
                    "if entity_name == \"rmhs\":\n",
                    "    columns_to_exclude = ['g','h','i','j','u','v','w','x','aj','ak','al','am']\n",
                    "else: \n",
                    "    columns_to_exclude = \"\"\n",
                    "\n",
                    "# get app id and timestamp from filename\n",
                    "parts = file_name.split('-')\n",
                    "file_app_id = parts[0]\n",
                    "file_timestamp = parts[2]\n",
                    "\n",
                    "print(\"app id: \" + file_app_id)\n",
                    "print(\"file timestamp: \" + file_timestamp)"
                ],
                "execution_count": 66
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Prepare exception dataframe"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "if entity_name == \"rmhs\":\n",
                    "\n",
                    "  # Create schema for the exception dataframe\n",
                    "  exception_schema = StructType([\n",
                    "    StructField('app_id', StringType(), True),\n",
                    "    StructField('datetime_stamp', StringType(), True),\n",
                    "    StructField('collection_name', StringType(), True),\n",
                    "    StructField('collection_description', StringType(), True),\n",
                    "    StructField('edit', StringType(), True),   \n",
                    "    StructField('a_service_id', StringType(), True),\n",
                    "    StructField('b_date_of_referral', StringType(), True),\n",
                    "    StructField('k_person_id', StringType(), True),\n",
                    "    StructField('validation_check', StringType(), True),\n",
                    "    StructField('data_element', StringType(), True),\n",
                    "    StructField('data_element_value', StringType(), True),\n",
                    "    StructField('severity', StringType(), True),    \n",
                    "    StructField('check_id', StringType(), True)\n",
                    "    ])\n",
                    "\n",
                    "elif entity_name == \"poc\":\n",
                    "\n",
                    "  exception_schema = StructType([\n",
                    "    StructField('app_id', StringType(), True),\n",
                    "    StructField('datetime_stamp', StringType(), True),\n",
                    "    StructField('collection_name', StringType(), True),\n",
                    "    StructField('collection_description', StringType(), True),     \n",
                    "    StructField('edit', StringType(), True),\n",
                    "    StructField('a_service_id', StringType(), True),\n",
                    "    StructField('b_person_id', StringType(), True),\n",
                    "    StructField('c_episode_id', StringType(), True),   \n",
                    "    StructField('d_poc_id', StringType(), True),         \n",
                    "    StructField('validation_check', StringType(), True), \n",
                    "    StructField('data_element', StringType(), True),\n",
                    "    StructField('data_element_value', StringType(), True),\n",
                    "    StructField('severity', StringType(), True),    \n",
                    "    StructField('check_id', StringType(), True)\n",
                    "    ])\n",
                    "\n",
                    "else: \n",
                    "    print(\"Unknown input file type: \" + entity_name) \n",
                    "    raise ValueError(\"Unknown input file type. Please investigate\")\n",
                    "\n",
                    "# Create empty dataframe for the exceptions\n",
                    "exception_df = spark.createDataFrame([], exception_schema)"
                ],
                "execution_count": 67
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Prepare schema"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# Create schema from file\n",
                    "schema_df = spark.read.option(\"multiline\", \"true\").json(schema_path)\n",
                    "schema_df = schema_df.toPandas()\n",
                    "\n",
                    "fields = []\n",
                    "\n",
                    "for row in range(len(schema_df)):\n",
                    "    field_name = schema_df.iloc[row][\"name\"]\n",
                    "    if schema_df.iloc[row][\"type\"] == \"integer\":\n",
                    "        field_type = IntegerType()\n",
                    "    elif schema_df.iloc[row][\"type\"] == \"date\":\n",
                    "        field_type = DateType()\n",
                    "    elif schema_df.iloc[row][\"type\"] == \"decimal\":\n",
                    "        precision = schema_df.iloc[row][\"precision\"] \n",
                    "        scale = schema_df.iloc[row][\"scale\"]\n",
                    "        field_type = DecimalType(precision = precision, scale = scale)\n",
                    "    else :\n",
                    "        field_type = StringType()\n",
                    "\n",
                    "    if field_name not in columns_to_exclude:\n",
                    "        fields.append(StructField(field_name, field_type))\n",
                    "\n",
                    "schema = StructType(fields)"
                ],
                "execution_count": 68
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Prepare data frames for template and header"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# Create a template dataframe with the expected schema\n",
                    "template_df = spark.read.schema(schema).option(\"header\", \"true\").csv(template_path)\n",
                    "\n",
                    "# Create a header-only dataframe from the template file\n",
                    "header_df = spark.read.option(\"header\", \"true\").csv(template_path)\n",
                    "\n",
                    "print(\"template_df and header_df created\")"
                ],
                "execution_count": 69
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# RAW - Perform initial validation"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Prepare data frame for input file"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# Create input_df\n",
                    "input_df = spark.read.option(\"header\", \"false\").csv(raw_input_file_path)\n",
                    "input_df = input_df.fillna(\"\")"
                ],
                "execution_count": 70
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Add file header from template if missing"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# Format the header row\n",
                    "# This function checks the first value in a csv file. \n",
                    "# If the value is a 3 digit integer, than we assume the header row is missing \n",
                    "# and the function will insert a header row from the template file.\n",
                    "# If it is not a 3 digit integer, than we assume there is a header row. \n",
                    "# In that case it will be overwrittent by the header row from the template file to ensure the column names are correct\n",
                    "\n",
                    "# Capture value of first cell\n",
                    "first_cell = input_df.collect()[0][0]\n",
                    "\n",
                    "# Check if first cell is an integer value or not\n",
                    "if bool(re.match(\"^\\d+$\", first_cell)) == True:\n",
                    "\n",
                    "    # Combine header_df and input_df as output_df\n",
                    "    print(\"Input file does not have a header row. Using the template.\")\n",
                    "    output_df = header_df.union(input_df)\n",
                    "\n",
                    "else:\n",
                    "    # Combine header_df and input_df with top row removed\n",
                    "    print(\"Added header to input file.\")\n",
                    "    output_df = header_df\\\n",
                    "        .union(input_df.where(input_df._c0!=first_cell))\\\n",
                    "        .union(input_df.where(input_df._c0.isNull()))\n",
                    "\n",
                    "print(\"Rows before filter applied: \" + str(output_df.count()))\n",
                    "\n",
                    "# Dynamically build the filter condition\n",
                    "condition = reduce(lambda a, b: a | b, [(trim(col(c)) != \"\") for c in output_df.columns])\n",
                    "\n",
                    "# Filter out rows where all columns are empty or contain only spaces\n",
                    "output_df = output_df.filter(condition)\n",
                    "\n",
                    "print(\"Rows after filter applied: \" + str(output_df.count()))\n",
                    "\n",
                    "# Write output to the designated path\n",
                    "output_df.toPandas().to_csv(raw_formatted_file_path, header=True, index=False)\n",
                    "\n",
                    "# Load output into dataframe formatted_df, not infering the schema from each column's values\n",
                    "formatted_df = spark.read.option(\"header\", \"true\").option(\"inferschema\", \"false\").csv(raw_formatted_file_path)"
                ],
                "execution_count": 71
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Check number of columns in input file"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# Make sure the number of columns in the input and schema data frames are the same\n",
                    "file_column_cnt = len(formatted_df.columns)\n",
                    "schema_column_cnt = len(schema.fields)\n",
                    "excluded_columns = len(columns_to_exclude)\n",
                    "\n",
                    "if file_column_cnt - excluded_columns != schema_column_cnt:\n",
                    "    print(\"Incorrect number of columns in file: \" + str(file_column_cnt) + \" (\" + str(schema_column_cnt) + \")\")"
                ],
                "execution_count": 72
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Check for missing primary key fields"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "if entity_name == \"rmhs\":\n",
                    "    df_missing_pk = formatted_df.filter(formatted_df.a_service_id.isNull() |\\\n",
                    "        formatted_df.b_date_of_referral.isNull() |\\\n",
                    "        formatted_df.k_person_id.isNull()\\\n",
                    "    )\n",
                    "elif entity_name == \"poc\":\n",
                    "    df_missing_pk = formatted_df.filter(formatted_df.a_service_id.isNull() |\\\n",
                    "        formatted_df.b_person_id.isNull() |\\\n",
                    "        formatted_df.c_episode_id.isNull() |\\\n",
                    "        formatted_df.d_poc_id.isNull()\\\n",
                    "        )\n",
                    "else: \n",
                    "    print(\"Unknown input file type: \" + entity_name) \n",
                    "    raise ValueError(\"Unknown input file type. Please investigate\")\n",
                    "\n",
                    "if df_missing_pk.count() > 0:\n",
                    "    print(str(df_missing_pk.count()) + \" rows are missing Identifier values\")\n",
                    "\n",
                    "    # Loop through each row of df_missing_pk\n",
                    "    for row in df_missing_pk.toLocalIterator():\n",
                    "\n",
                    "        if entity_name == \"rmhs\":\n",
                    "\n",
                    "            c_a_service_id = row['a_service_id'] if row['a_service_id'] else \"\"\n",
                    "            c_b_date_of_referral = row['b_date_of_referral'] if row['b_date_of_referral'] else \"\"\n",
                    "            c_k_person_id = row['k_person_id'] if row['k_person_id'] else \"\"\n",
                    "            c_data_element = \"[A Service ID] | [B Date of referral] | [K Person ID]\"\n",
                    "            c_data_element_value = c_a_service_id + \" | \" + c_b_date_of_referral + \" | \" + c_k_person_id\n",
                    "\n",
                    "            # Create a new rmhs row in exception data frame\n",
                    "            new_exception = Row( \\\n",
                    "                app_id = file_app_id, \\\n",
                    "                datetime_stamp = file_timestamp, \\\n",
                    "                collection_name = \"rmhs\", \\\n",
                    "                collection_description = \"patient episode\", \\\n",
                    "                edit = \"01\", \\\n",
                    "                a_service_id = c_a_service_id, \\\n",
                    "                b_date_of_referral = c_b_date_of_referral, \\\n",
                    "                k_person_id = c_k_person_id, \\\n",
                    "                validation_check = \"Identifier value is required\", \\\n",
                    "                data_element = c_data_element, \\\n",
                    "                data_element_value = c_data_element_value, \\\n",
                    "                severity=\"Critical\", \\\n",
                    "                check_id = \"di_pk\")\n",
                    "\n",
                    "        elif entity_name == \"poc\":\n",
                    "\n",
                    "            c_a_service_id = row['a_service_id'] if row['a_service_id'] else \"\"\n",
                    "            c_b_person_id = row['b_person_id'] if row['b_person_id'] else \"\"\n",
                    "            c_c_episode_id = row['c_episode_id'] if row['c_episode_id'] else \"\"\n",
                    "            d_poc_id = row['d_poc_id'] if row['d_poc_id'] else \"\"\n",
                    "            c_data_element = \"[A Service ID] | [B Person ID] | [C Episode ID] | [D Phase of Care ID]\"\n",
                    "            c_data_element_value = c_a_service_id + \" | \" + c_b_person_id + \" | \" + c_c_episode_id + \" | \" + d_poc_id                     \n",
                    "\n",
                    "            new_exception = Row( \\\n",
                    "                app_id = file_app_id, \\\n",
                    "                datetime_stamp = file_timestamp, \\\n",
                    "                collection_name = \"rmhs\", \\\n",
                    "                collection_description = \"phase of care\", \\\n",
                    "                edit = \"01\", \\\n",
                    "                a_service_id = c_a_service_id, \\\n",
                    "                b_person_id = c_b_person_id, \\\n",
                    "                c_episode_id = c_c_episode_id, \\\n",
                    "                poc_id = d_poc_id, \\\n",
                    "                validation_check = \"Identifier value is required\", \\\n",
                    "                data_element = c_data_element, \\\n",
                    "                data_element_value = c_data_element_value, \\\n",
                    "                severity=\"Critical\", \\\n",
                    "                check_id = \"di_pk\")\n",
                    "\n",
                    "        # Convert the new row to a DataFrame\n",
                    "        new_exception_df = spark.createDataFrame([new_exception])\n",
                    "\n",
                    "        # Append the new row to the existing DataFrame\n",
                    "        exception_df = exception_df.union(new_exception_df)\n",
                    "\n",
                    "    print(\"Exceptions so far: \" + str(exception_df.count()))"
                ],
                "execution_count": 73
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Check for duplicate primary key values"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "if entity_name == \"rmhs\":\n",
                    "    # Check for duplicated episode rows - PK: a_service_id, b_referral_date or k_person_id\n",
                    "    duplicated_episode_rows = formatted_df.groupBy(\"a_service_id\", \"b_date_of_referral\", \"k_person_id\") \\\n",
                    "        .count() \\\n",
                    "        .filter(col(\"count\") > 1).distinct()\n",
                    "\n",
                    "    df_duplicated = formatted_df.join(duplicated_episode_rows, on=[\"a_service_id\", \"b_date_of_referral\", \"k_person_id\"], how=\"inner\").orderBy(formatted_df.k_person_id.desc())\n",
                    "\n",
                    "elif entity_name == \"poc\":\n",
                    "    # Check for duplicated episode rows - PK: a_service_id, b_person_id, c_episode_id or d_poc_id\n",
                    "    duplicated_poc_rows = formatted_df.groupBy(\"a_service_id\", \"b_person_id\", \"c_episode_id\", \"d_poc_id\") \\\n",
                    "        .count() \\\n",
                    "        .filter(col(\"count\") > 1).distinct()\n",
                    "\n",
                    "    df_duplicated = formatted_df.join(duplicated_poc_rows, on=[\"a_service_id\", \"b_person_id\", \"c_episode_id\", \"d_poc_id\"], how=\"inner\").orderBy(formatted_df.b_person_id.desc())\n",
                    "else: \n",
                    "    print(\"Unknown input file type: \" + entity_name) \n",
                    "    raise ValueError(\"Unknown input file type. Please investigate\")\n",
                    "\n",
                    "if df_duplicated.count() > 0:\n",
                    "    print(\"Duplicated Identifier row(s): \" + str(df_duplicated.count()))\n",
                    "\n",
                    "    # Loop through each row of df_duplicated\n",
                    "    for row in df_duplicated.toLocalIterator():\n",
                    "\n",
                    "        if entity_name == \"rmhs\":\n",
                    "\n",
                    "            c_a_service_id = row['a_service_id'] if row['a_service_id'] else \"\"\n",
                    "            c_b_date_of_referral = row['b_date_of_referral'] if row['b_date_of_referral'] else \"\"\n",
                    "            c_k_person_id = row['k_person_id'] if row['k_person_id'] else \"\"\n",
                    "            c_data_element = \"[A Service ID] | [B Date of referral] | [K Person ID]\"\n",
                    "            c_data_element_value = c_a_service_id + \" | \" + c_b_date_of_referral + \" | \" + c_k_person_id            \n",
                    "\n",
                    "            # Create a new rmhs row in exception data frame\n",
                    "            new_exception = Row( \\\n",
                    "                app_id = file_app_id, \\\n",
                    "                datetime_stamp = file_timestamp, \\\n",
                    "                collection_name = \"rmhs\", \\\n",
                    "                collection_description = \"patient episode\", \\\n",
                    "                edit = \"23\", \\\n",
                    "                a_service_id = c_a_service_id, \\\n",
                    "                b_date_of_referral = c_b_date_of_referral, \\\n",
                    "                k_person_id = c_k_person_id, \\\n",
                    "                validation_check = \"Value should be unique to the service\", \\\n",
                    "                data_element = c_data_element, \\\n",
                    "                data_element_value = c_data_element_value, \\\n",
                    "                severity=\"Critical\", \\\n",
                    "                check_id = \"di_pk\")\n",
                    "\n",
                    "        elif entity_name == \"poc\":\n",
                    "\n",
                    "            c_a_service_id = row['a_service_id'] if row['a_service_id'] else \"\"\n",
                    "            c_b_person_id = row['b_person_id'] if row['b_person_id'] else \"\"\n",
                    "            c_c_episode_id = row['c_episode_id'] if row['c_episode_id'] else \"\"\n",
                    "            d_poc_id = row['d_poc_id'] if row['d_poc_id'] else \"\" \n",
                    "            c_data_element = \"[A Service ID] | [B Person ID] | [C Episode ID] | [D Phase of Care ID]\"\n",
                    "            c_data_element_value = c_a_service_id + \" | \" + c_b_person_id + \" | \" + c_c_episode_id + \" | \" + d_poc_id                       \n",
                    "\n",
                    "            # Create a new phase of care row in exception data frame\n",
                    "            new_exception = Row( \\\n",
                    "                app_id = file_app_id, \\\n",
                    "                datetime_stamp = file_timestamp, \\\n",
                    "                collection_name = \"rmhs\", \\\n",
                    "                collection_description = \"phase of care\", \\\n",
                    "                edit = \"23\", \\\n",
                    "                a_service_id = c_a_service_id, \\\n",
                    "                b_person_id = c_b_person_id, \\\n",
                    "                c_episode_id = c_c_episode_id, \\\n",
                    "                poc_id = d_poc_id, \\\n",
                    "                validation_check = \"Value should be unique to the service\", \\\n",
                    "                data_element = c_data_element, \\\n",
                    "                data_element_value = c_data_element_value, \\\n",
                    "                severity=\"Critical\", \\\n",
                    "                check_id = \"di_pk\")\n",
                    "\n",
                    "        # Convert the new row to a DataFrame\n",
                    "        new_exception_df = spark.createDataFrame([new_exception])\n",
                    "\n",
                    "        # Append the new row to the existing DataFrame\n",
                    "        exception_df = exception_df.union(new_exception_df)\n",
                    "\n",
                    "    print(\"Exceptions so far: \" + str(exception_df.count()))"
                ],
                "execution_count": 74
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Manage notebook activity in pipeline"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# Raise error and \"fail\" the notebook if rows in exception_df > 0\n",
                    "if exception_df.count() > 0:\n",
                    "\n",
                    "    if entity_name == \"rmhs\":\n",
                    "        exception_df = exception_df.orderBy(col(\"severity\").asc(), col(\"k_person_id\").asc(), col(\"edit\").asc())        \n",
                    "\n",
                    "    elif entity_name == \"poc\":\n",
                    "        exception_df = exception_df.orderBy(col(\"severity\").asc(), col(\"b_person_id\").asc(), col(\"edit\").asc())\n",
                    "    \n",
                    "    # Save exceptions to csv file\n",
                    "    exception_df.toPandas().to_csv(exception_file_path, header=True, index=False)\n",
                    "    print(\"Total number of exceptions: \" + str(exception_df.count()))\n",
                    "\n",
                    "    # terminate the notebook with error\n",
                    "    raise ValueError(\"Initial validation of submitted file failed, please see validation report\")    \n",
                    "\n",
                    "else: \n",
                    "    print(\"End of validation checks\")"
                ],
                "execution_count": 75
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Save file in stg_dataimport folder in a delta format"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    },
                    "collapsed": false
                },
                "source": [
                    "# create export_df dataframe with all columns cast as string\n",
                    "export_df = formatted_df.select([col(c).cast(\"string\") for c in formatted_df.columns])\n",
                    "\n",
                    "# write export_df as a .csv file, overwriting the original file\n",
                    "export_df.toPandas().to_csv(raw_input_file_path, header=True, index=False)\n",
                    "\n",
                    "# Read data from csv to spark dataframe\n",
                    "df = spark.read.option(\"header\", \"true\").csv(raw_formatted_file_path)\n",
                    "\n",
                    "# Remove existing Delta files - versioning is not needed\n",
                    "if (DeltaTable.isDeltaTable(spark, raw_delta_path)):\n",
                    "\n",
                    "    # Convert the path to a regular URI format\n",
                    "    uri = raw_delta_path.replace(\"abfss:/\", \"abfs:/\")\n",
                    "\n",
                    "    # Delete the folder using Hadoop FileSystem API\n",
                    "    os.system(f'hadoop fs -rm -r \"{uri}\" > NUL 2>&1')   \n",
                    "\n",
                    "# Write to location in delta format\n",
                    "df.write.format(\"delta\").mode(\"overwrite\").save(raw_delta_path + \"/\")\n",
                    "\n",
                    "# Count the number of rows in the DataFrame\n",
                    "print(\"Number of rows saved:\", df.count())"
                ],
                "execution_count": 76
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# BRONZE - Perform Data Transformation"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Load incoming data"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# Load input into a dataframe\n",
                    "output_df = spark.read.format(\"delta\").load(raw_delta_path)\n",
                    "\n",
                    "# remove empty columns\n",
                    "if entity_name == \"rmhs\":\n",
                    "    output_df = output_df.drop(*columns_to_exclude)\n",
                    "\n",
                    "print(\"Incomning rows: \" + str(output_df.count()))"
                ],
                "execution_count": 77
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Remove leading and trailing whitespace on every column"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# Trim all columns\n",
                    "output_df = output_df.select([trim(c).alias(c) for c in output_df.columns])\n",
                    "\n",
                    "print(\"Values trimmed\")"
                ],
                "execution_count": 78
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Update 4 digit codes where leading '0' is missing"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# Add missing leading '0's to specified 4 char lenghts columns\n",
                    "if entity_name == \"rmhs\": \n",
                    "    columns = [\"l_area_of_usual_residence\", \"q_country_of_birth\", \"r_main_language_other_than_english\"]\n",
                    "\n",
                    "    for c in columns:\n",
                    "        output_df = output_df.withColumn(c, func.substring_index(output_df[c], '.', 1).alias('left'))\n",
                    "        output_df = output_df.withColumn(c, func.lpad(output_df[c],4,'0'))\n",
                    "\n",
                    "    print(\"Missing leading '0' added to code columns\")"
                ],
                "execution_count": 79
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Update 6 digit codes where leading '0' is missing"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# Add missing leading '0's to specified 6 char lenght columns\n",
                    "columns = [\"a_service_id\"]\n",
                    "\n",
                    "for c in columns:\n",
                    "    output_df = output_df.withColumn(c, func.substring_index(output_df[c], '.', 1).alias('left'))\n",
                    "    output_df = output_df.withColumn(c, func.lpad(output_df[c],6,'0'))\n",
                    "\n",
                    "print(\"Missing leading '0' added to code columns\")"
                ],
                "execution_count": 80
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Remove leading zeroes on string values"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# Remove leading '0's from char columns (leading zeroes added by formatting rules in Excel)\n",
                    "if entity_name == \"rmhs\": \n",
                    "    columns = [\"y_episode_id\"]\n",
                    "else:\n",
                    "    columns = [\"c_episode_id\"]\n",
                    "\n",
                    "for c in columns:\n",
                    "    output_df = output_df.withColumn(\n",
                    "        c,\n",
                    "        func.when(func.col(c) == \"0\", \"0\")  # Keep \"0\" as is\n",
                    "        .otherwise(func.regexp_replace(func.col(c), r\"^0+\", \"\"))\n",
                    "    )\n",
                    "\n",
                    "print(\"Removed leading '0' from the Episode ID\")"
                ],
                "execution_count": 81
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## CAPITLIZE columns for consistency"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# Capitalize specified columns\n",
                    "if entity_name == \"rmhs\":\n",
                    "    columns = [\"k_person_id\",\"t_umrn\"]\n",
                    "elif entity_name == \"poc\":\n",
                    "    columns = [\"b_person_id\"]\n",
                    "else: \n",
                    "    print(\"Unknown input file type: \" + entity_name) \n",
                    "    columns = \"[]\"\n",
                    "    raise ValueError(\"Unknown input file type. Please investigate\")\n",
                    "\n",
                    "for c in columns:\n",
                    "    output_df = output_df.withColumn(c, func.upper(output_df[c]))\n",
                    "\n",
                    "print(\"Capitalized specified columns\")"
                ],
                "execution_count": 82
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Update `dates` where leading '0' is missing"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# Add missing leading '0's to specified date columns\n",
                    "if entity_name == \"rmhs\": \n",
                    "    columns = [\"b_date_of_referral\", \"c_date_of_assessment\", \"m_date_of_birth\", \"z_start_date\", \"an_end_date\"]\n",
                    "elif entity_name == \"poc\":\n",
                    "    columns = [\"f_poc_start_date\", \"g_poc_end_date\"]\n",
                    "else: \n",
                    "    print(\"Unknown input file type: \" + entity_name) \n",
                    "    raise ValueError(\"Unknown input file type. Please investigate\")\n",
                    "\n",
                    "for c in columns:\n",
                    "    output_df = output_df.withColumn(c, func.substring_index(output_df[c], '.', 1).alias('left'))\n",
                    "    output_df = output_df.withColumn(c, func.lpad(output_df[c],8,'0'))\n",
                    "\n",
                    "# Convert date columns to expected format\n",
                    "for c in columns:\n",
                    "    output_df = output_df.withColumn(c,\\\n",
                    "        concat_ws('-',\\\n",
                    "            substring(output_df[c], -4, 4),\\\n",
                    "            substring(output_df[c], -6, 2),\\\n",
                    "            substring(output_df[c], -8, 2)\\\n",
                    "        )\\\n",
                    "    )\n",
                    "\n",
                    "print(\"Missing leading '0' added to date columns\")"
                ],
                "execution_count": 83
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Report on dates in incorrect format"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# Create a temporary view to use SQL queries\n",
                    "output_df.createOrReplaceTempView(\"dates_table\")\n",
                    "\n",
                    "if entity_name == \"rmhs\":\n",
                    "        \n",
                    "    result_df = spark.sql(\"\"\"\n",
                    "        SELECT \n",
                    "            a_service_id, b_date_of_referral, k_person_id, 'b_date_of_referral' AS data_element, b_date_of_referral AS data_element_value,\n",
                    "            CASE WHEN b_date_of_referral = '' THEN 'invalid' WHEN to_date(b_date_of_referral, 'yyyy-MM-dd') IS NOT NULL THEN 'valid' ELSE 'invalid' END AS date_check          \n",
                    "        FROM dates_table\n",
                    "\n",
                    "        UNION\n",
                    "\n",
                    "        SELECT \n",
                    "            a_service_id, b_date_of_referral, k_person_id, 'c_date_of_assessment' AS data_element, c_date_of_assessment AS data_element_value,\n",
                    "            CASE WHEN c_date_of_assessment = '' THEN 'invalid' WHEN to_date(c_date_of_assessment, 'yyyy-MM-dd') IS NOT NULL THEN 'valid' ELSE 'invalid' END AS date_check          \n",
                    "        FROM dates_table\n",
                    "\n",
                    "        UNION\n",
                    "\n",
                    "        SELECT \n",
                    "            a_service_id, b_date_of_referral, k_person_id, 'm_date_of_birth' AS data_element, m_date_of_birth AS data_element_value,\n",
                    "            CASE WHEN m_date_of_birth = '' THEN 'invalid' WHEN to_date(m_date_of_birth, 'yyyy-MM-dd') IS NOT NULL THEN 'valid' ELSE 'invalid' END AS date_check          \n",
                    "        FROM dates_table\n",
                    "\n",
                    "        UNION\n",
                    "\n",
                    "        SELECT \n",
                    "            a_service_id, b_date_of_referral, k_person_id, 'z_start_date' AS data_element, z_start_date AS data_element_value,\n",
                    "            CASE WHEN z_start_date = '' THEN 'valid' WHEN to_date(z_start_date, 'yyyy-MM-dd') IS NOT NULL THEN 'valid' ELSE 'invalid' END AS date_check          \n",
                    "        FROM dates_table\n",
                    "\n",
                    "        UNION\n",
                    "\n",
                    "        SELECT \n",
                    "            a_service_id, b_date_of_referral, k_person_id, 'an_end_date' AS data_element, an_end_date AS data_element_value,\n",
                    "            CASE WHEN an_end_date = '' THEN 'valid' WHEN to_date(an_end_date, 'yyyy-MM-dd') IS NOT NULL THEN 'valid' ELSE 'invalid' END AS date_check          \n",
                    "        FROM dates_table\n",
                    "    \"\"\")\n",
                    "\n",
                    "    result_df = result_df.filter(F.col('date_check') == 'invalid').select(\"a_service_id\", \"b_date_of_referral\", \"k_person_id\", \"data_element\", \"data_element_value\")\n",
                    "    #result_df.show()\n",
                    "\n",
                    "elif entity_name == \"poc\":\n",
                    "\n",
                    "    result_df = spark.sql(\"\"\"\n",
                    "        SELECT \n",
                    "            a_service_id, b_person_id, c_episode_id, d_poc_id, 'f_poc_start_date' AS data_element, f_poc_start_date AS data_element_value,\n",
                    "            CASE WHEN f_poc_start_date = '' THEN 'invalid' WHEN to_date(f_poc_start_date, 'yyyy-MM-dd') IS NOT NULL THEN 'valid' ELSE 'invalid' END AS date_check          \n",
                    "        FROM dates_table\n",
                    "\n",
                    "        UNION\n",
                    "\n",
                    "        SELECT \n",
                    "            a_service_id, b_person_id, c_episode_id, d_poc_id, 'g_poc_end_date' AS data_element, g_poc_end_date AS data_element_value,\n",
                    "            CASE WHEN g_poc_end_date = '' THEN 'valid' WHEN to_date(g_poc_end_date, 'yyyy-MM-dd') IS NOT NULL THEN 'valid' ELSE 'invalid' END AS date_check          \n",
                    "        FROM dates_table\n",
                    "    \"\"\")\n",
                    "\n",
                    "    result_df = result_df.filter(F.col('date_check') == 'invalid').select(\"a_service_id\", \"b_person_id\", \"c_episode_id\", \"d_poc_id\", \"data_element\", \"data_element_value\")\n",
                    "    #result_df.show()\n",
                    "\n",
                    "for row in result_df.collect():\n",
                    "\n",
                    "    if entity_name == \"rmhs\":\n",
                    "\n",
                    "        new_exception = Row( \\\n",
                    "            app_id = file_app_id, \\\n",
                    "            datetime_stamp = file_timestamp, \\\n",
                    "            collection_name = \"rmhs\", \\\n",
                    "            collection_description = \"patient episode\", \\\n",
                    "            edit = \"04\", \\\n",
                    "            a_service_id = f\"{row['a_service_id']}\", \\\n",
                    "            b_date_of_referral = f\"{row['b_date_of_referral']}\", \\\n",
                    "            k_person_id = f\"{row['k_person_id']}\", \\\n",
                    "            validation_check = \"Value submitted is not in the required format\", \\\n",
                    "            data_element = f\"{row['data_element']}\", \\\n",
                    "            data_element_value = f\"{row['data_element_value']}\", \\\n",
                    "            severity=\"Critical\", \\\n",
                    "            check_id = \"date_format\")     \n",
                    "\n",
                    "    elif entity_name == \"poc\":\n",
                    "\n",
                    "        new_exception = Row( \\\n",
                    "            app_id = file_app_id, \\\n",
                    "            datetime_stamp = file_timestamp, \\\n",
                    "            collection_name = \"rmhs\", \\\n",
                    "            collection_description = \"phase of care\", \\\n",
                    "            edit = \"04\", \\\n",
                    "            a_service_id = f\"{row['a_service_id']}\", \\\n",
                    "            b_person_id = f\"{row['b_person_id']}\", \\\n",
                    "            c_episode_id = f\"{row['c_episode_id']}\", \\\n",
                    "            d_poc_id = f\"{row['d_poc_id']}\", \\\n",
                    "            validation_check = \"Value submitted is not in the required format\", \\\n",
                    "            data_element = f\"{row['data_element']}\", \\\n",
                    "            data_element_value = f\"{row['data_element_value']}\", \\\n",
                    "            severity=\"Critical\", \\\n",
                    "            check_id = \"di_format\")\n",
                    "\n",
                    "    # Convert the new row to a DataFrame\n",
                    "    new_exception_df = spark.createDataFrame([new_exception])\n",
                    "\n",
                    "    # Append the new row to the existing DataFrame\n",
                    "    exception_df = exception_df.union(new_exception_df)\n",
                    "\n",
                    "print(\"Exceptions so far: \" + str(exception_df.count()))"
                ],
                "execution_count": 84
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Cast non-date data to correct data type"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "formatted_df = output_df\n",
                    "\n",
                    "# Report any conversion errors\n",
                    "for column in fields:\n",
                    "    col_name = column.name\n",
                    "    col_type = column.dataType\n",
                    "\n",
                    "\t# Skip columns that are of DateType\n",
                    "    if isinstance(col_type, DateType):\n",
                    "        continue  # Excludes date columns from the check\n",
                    "\n",
                    "    # Store original value\n",
                    "    formatted_df = formatted_df.withColumn(\n",
                    "        f\"{col_name}_original\", \n",
                    "        col(col_name)\n",
                    "    )\n",
                    "\n",
                    "    formatted_df = formatted_df.withColumn(\n",
                    "        col_name, \n",
                    "        when(col(col_name).isNotNull(), col(col_name).cast(col_type))\n",
                    "        .otherwise(None)  # Keep NULL values intact\n",
                    "    )\n",
                    "\n",
                    "    # Check for values that could not be converted to the data type listed in the schema file\n",
                    "    formatted_df = formatted_df.withColumns({\n",
                    "        \"error_column\": when(\n",
                    "            col(f\"{col_name}_original\").isNotNull() & col(col_name).isNull(), f\"{col_name}\"\n",
                    "        ).otherwise(None),\n",
                    "        \n",
                    "        \"error_value\": when(\n",
                    "            col(f\"{col_name}_original\").isNotNull() & col(col_name).isNull(), col(f\"{col_name}_original\")\n",
                    "        ).otherwise(None)\n",
                    "    })\n",
                    "\n",
                    "    # Get errors\n",
                    "    error_df = formatted_df.filter(col(\"error_column\").isNotNull())\n",
                    "    if error_df.count() > 0:\n",
                    "        print(f\"Number of conversion errors for column {col_name}: \" + str(error_df.count()))\n",
                    "\n",
                    "    # Collect rows from error_df\n",
                    "    error_rows = error_df.collect()\n",
                    "\n",
                    "    # Loop through the errors and add to exception dataframe\n",
                    "    for row in error_rows:\n",
                    "        incorrect_value = f\"{row['error_value']}\"\n",
                    "\n",
                    "        if entity_name == \"rmhs\":\n",
                    "\n",
                    "            new_exception = Row( \\\n",
                    "                app_id = file_app_id, \\\n",
                    "                datetime_stamp = file_timestamp, \\\n",
                    "                collection_name = \"rmhs\", \\\n",
                    "                collection_description = \"patient episode\", \\\n",
                    "                edit = \"04\", \\\n",
                    "                a_service_id = \"\", \\\n",
                    "                b_date_of_referral = \"\", \\\n",
                    "                k_person_id = \" \", \\\n",
                    "                validation_check = \"Value submitted is not in the required format\", \\\n",
                    "                data_element = col_name, \\\n",
                    "                data_element_value = incorrect_value, \\\n",
                    "                severity=\"Critical\", \\\n",
                    "                check_id = \"column_format\")    \n",
                    "\n",
                    "        elif entity_name == \"poc\":      \n",
                    "\n",
                    "            new_exception = Row( \\\n",
                    "                app_id = file_app_id, \\\n",
                    "                datetime_stamp = file_timestamp, \\\n",
                    "                collection_name = \"rmhs\", \\\n",
                    "                collection_description = \"phase of care\", \\\n",
                    "                edit = \"04\", \\\n",
                    "                a_service_id = \"\", \\\n",
                    "                b_person_id = \" \", \\\n",
                    "                c_episode_id = \"\", \\\n",
                    "                poc_id = \"\", \\\n",
                    "                validation_check = \"Value submitted is not in the required format\", \\\n",
                    "                data_element = col_name, \\\n",
                    "                data_element_value = incorrect_value, \\\n",
                    "                severity=\"Critical\", \\\n",
                    "                check_id = \"column_format\")\n",
                    "\n",
                    "        # Convert the new row to a DataFrame\n",
                    "        new_exception_df = spark.createDataFrame([new_exception])\n",
                    "\n",
                    "        # Append the new row to the existing DataFrame\n",
                    "        exception_df = exception_df.union(new_exception_df)\n",
                    "\n",
                    "# Show all errors\n",
                    "exception_df.show(truncate=False)\n",
                    "\n",
                    "# Convert the data if no errors were found\n",
                    "if exception_df.count() == 0:\n",
                    "\n",
                    "    error_df = output_df\n",
                    "\n",
                    "    for field in fields:   #schema.fields:\n",
                    "        col_name = field.name\n",
                    "        col_type = field.dataType\n",
                    "\n",
                    "        try:\n",
                    "            output_df = output_df.withColumn(col_name, output_df[col_name].cast(col_type))\n",
                    "            error_df = error_df.drop(col_name)\n",
                    "\n",
                    "        except Exception as e:\n",
                    "            print(\"All values in column \" + col_name + \" cannot be converted to the correct data type.\")\n",
                    "            print(e)\n",
                    "\n",
                    "            if entity_name == \"rmhs\":\n",
                    "\n",
                    "                new_exception = Row( \\\n",
                    "                    app_id = file_app_id, \\\n",
                    "                    datetime_stamp = file_timestamp, \\\n",
                    "                    collection_name = \"rmhs\", \\\n",
                    "                    collection_description = \"patient episode\", \\\n",
                    "                    edit = \"04\", \\\n",
                    "                    a_service_id = \"\", \\\n",
                    "                    b_date_of_referral = \"\", \\\n",
                    "                    k_person_id = \" \", \\\n",
                    "                    validation_check = \"Value submitted in column \" + col_name + \" is not in the required format\", \\\n",
                    "                    data_element = col_name, \\\n",
                    "                    data_element_value = \"\", \\\n",
                    "                    severity=\"Critical\", \\\n",
                    "                    check_id = \"di_format\")          \n",
                    "\n",
                    "            elif entity_name == \"poc\":      \n",
                    "\n",
                    "                new_exception = Row( \\\n",
                    "                    app_id = file_app_id, \\\n",
                    "                    datetime_stamp = file_timestamp, \\\n",
                    "                    collection_name = \"rmhs\", \\\n",
                    "                    collection_description = \"phase of care\", \\\n",
                    "                    edit = \"04\", \\\n",
                    "                    a_service_id = \"\", \\\n",
                    "                    b_person_id = \" \", \\\n",
                    "                    c_episode_id = \"\", \\\n",
                    "                    poc_id = \"\", \\\n",
                    "                    validation_check = \"Value submitted in column \" + col_name + \" is not in the required format\", \\\n",
                    "                    data_element = col_name, \\\n",
                    "                    data_element_value = \"\", \\\n",
                    "                    severity=\"Critical\", \\\n",
                    "                    check_id = \"di_format\")\n",
                    "\n",
                    "            # Convert the new row to a DataFrame\n",
                    "            new_exception_df = spark.createDataFrame([new_exception])\n",
                    "\n",
                    "            # Append the new row to the existing DataFrame\n",
                    "            exception_df = exception_df.union(new_exception_df)\n",
                    "\n",
                    "            continue\n",
                    "\n",
                    "print(\"Exceptions so far: \" + str(exception_df.count()))"
                ],
                "execution_count": 85
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Manage notebook activity in pipeline"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# Raise error and \"fail\" the notebook if rows in exception_df > 0\n",
                    "if exception_df.count() > 0:\n",
                    "\n",
                    "    if entity_name == \"rmhs\":\n",
                    "        exception_df = exception_df.orderBy(col(\"severity\").asc(), col(\"k_person_id\").asc(), col(\"edit\").asc())        \n",
                    "\n",
                    "    elif entity_name == \"poc\":\n",
                    "        exception_df = exception_df.orderBy(col(\"severity\").asc(), col(\"b_person_id\").asc(), col(\"edit\").asc())\n",
                    "    \n",
                    "    # Save exceptions to csv file\n",
                    "    exception_df.toPandas().to_csv(exception_file_path, header=True, index=False)\n",
                    "    print(\"Total number of exceptions: \" + str(exception_df.count()))\n",
                    "\n",
                    "    # terminate the notebook with error\n",
                    "    raise ValueError(\"The data in the submitted file could't not be transformed into correct format, please see validation report\")    \n",
                    "\n",
                    "else: \n",
                    "    print(\"Transformation of data was successful\")"
                ],
                "execution_count": 86
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "### Add additional columns for row based validation"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# Add column: specification\n",
                    "df_specification = spark.sql(f\"SELECT max(ref) as current_spec FROM c2_rmhs.dim_specification\")\n",
                    "\n",
                    "# Extract values from DataFrames\n",
                    "current_specification = df_specification.collect()[0]['current_spec']\n",
                    "\n",
                    "output_df = output_df.withColumn(\"specification\", lit(current_specification)) \n",
                    "\n",
                    "if entity_name == \"rmhs\":\n",
                    "    # Add columns: program, service_name and ngo to make validation easier\n",
                    "    df_service = spark.sql(f\"SELECT uid, ref, description as service_name, program, ngo as organisation, valid_from, valid_to FROM c2_rmhs.dim_service\")\n",
                    "    df_service.createOrReplaceTempView(\"c2_rmhs_dim_service\")\n",
                    "\n",
                    "    output_df.createOrReplaceTempView(\"original_table\")\n",
                    "\n",
                    "    output_df = spark.sql(\"\"\"\n",
                    "        SELECT o.*, s.service_name, s.program, s.organisation\n",
                    "        FROM original_table o\n",
                    "        LEFT JOIN c2_rmhs_dim_service s\n",
                    "        ON o.a_service_id = s.ref\n",
                    "    \"\"\")"
                ],
                "execution_count": 87
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Store data as delta file in stg_validation location"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# Remove existing Delta files - versioning is not needed\n",
                    "if (DeltaTable.isDeltaTable(spark, bronze_delta_path)):\n",
                    "\n",
                    "    # Convert the path to a regular URI format\n",
                    "    uri = bronze_delta_path.replace(\"abfss:/\", \"abfs:/\")\n",
                    "\n",
                    "    # Delete the folder using Hadoop FileSystem API\n",
                    "    os.system(f'hadoop fs -rm -r \"{uri}\" > NUL 2>&1')   \n",
                    "\n",
                    "# Write to location in delta format\n",
                    "output_df.write.format(\"delta\").mode(\"overwrite\").save(bronze_delta_path)\n",
                    "\n",
                    "# Reset database\n",
                    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS `{target_database}`\")\n",
                    "spark.sql(f\"DROP TABLE IF EXISTS {target_database}.{validation_table}\")\n",
                    "\n",
                    "# Create lake table from delta location\n",
                    "spark.sql(f\"CREATE TABLE {target_database}.{validation_table} USING DELTA LOCATION '{bronze_delta_path}'\")\n",
                    "print(f\"Table {target_database}.{validation_table} created from delta location\")"
                ],
                "execution_count": 88
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Load data to be validated"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# Load validation rules into a data frame\n",
                    "rules_df = spark.read.option(\"multiline\", \"true\").json(rules_delta_path)\n",
                    "rulespd_df = rules_df.toPandas()\n",
                    "\n",
                    "# Load data to be validated into a data frame\n",
                    "data_validation_df = spark.read.format(\"delta\").load(bronze_delta_path)\n",
                    "print(\"Records to be validated: \" + str(data_validation_df.count()))"
                ],
                "execution_count": 89
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Perform occupancy validation"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "if entity_name == \"rmhs\":\n",
                    "\n",
                    "    # Load staging data into vw\n",
                    "    df_staging_data = spark.sql(f\"SELECT a_service_id, service_name as service, z_start_date, an_end_date FROM c2_rmhs.\" + validation_table + \" WHERE z_start_date IS NOT NULL\")\n",
                    "    df_staging_data.createOrReplaceTempView(validation_view)\n",
                    "\n",
                    "    # Create CTE to get average beds per Agency per month in current finacial year\n",
                    "    sql_query = \"\"\"\n",
                    "        WITH cte_GetAvgBeds AS (\n",
                    "            SELECT\n",
                    "                S.ref                                                       AS service_id,\n",
                    "                S.description                                               AS service,\n",
                    "                CAST(DATE_TRUNC('month', D.date) AS DATE)                   AS period_start,\n",
                    "                LAST_DAY(DATE)                                              AS period_end,\n",
                    "                CAST(AVG(CAST(S.beds AS DECIMAL(10, 2))) AS DECIMAL(4, 1))  AS avg_beds,\n",
                    "                YEAR(D.date)                                                AS year\n",
                    "            FROM c2_rmhs.dim_service AS S\n",
                    "                INNER JOIN \"\"\" + validation_view + \"\"\" AS STG ON STG.a_service_id = S.ref\n",
                    "                INNER JOIN common.dim_date AS D ON D.date BETWEEN S.valid_from AND COALESCE(S.valid_to, CURRENT_DATE())\n",
                    "            GROUP BY S.ref, S.description, DATE_TRUNC('MONTH', DATE), DATE, YEAR(D.date)\n",
                    "        )\n",
                    "        SELECT service_id, service, period_start, period_end, AVG(avg_beds) AS beds, year\n",
                    "        FROM cte_GetAvgBeds\n",
                    "        GROUP BY service_id, service, period_start, period_end, year\n",
                    "        ORDER BY service_id, period_start\n",
                    "        \"\"\"\n",
                    "\n",
                    "    # Execute the SQL query\n",
                    "    df_avg_beds = spark.sql(sql_query)\n",
                    "    df_avg_beds.createOrReplaceTempView(\"df_avg_beds\")\n",
                    "\n",
                    "    # Create CTE for month information\n",
                    "\n",
                    "    # Define the SQL query\n",
                    "    sql_query = \"\"\"\n",
                    "        WITH cte_month AS (\n",
                    "            SELECT\n",
                    "                ROW_NUMBER() OVER (ORDER BY D.calendar_month_number)    AS uid,\n",
                    "                D.calendar_month_number                                 AS month_number,\n",
                    "                D.month_name                                            AS month_name,\n",
                    "                D.days_in_month,\n",
                    "                MAX(D.days_calendar_ytd)                                AS days_cytd,\n",
                    "                MAX(D.days_fin_year_ytd)                                AS days_fytd\n",
                    "            FROM common.dim_date AS D\n",
                    "            WHERE D.leap_year = 'NO'\n",
                    "            GROUP BY D.calendar_month_number, D.month_name, D.days_in_month\n",
                    "        )\n",
                    "        SELECT * FROM cte_month\n",
                    "        \"\"\"\n",
                    "\n",
                    "    # Execute the SQL query\n",
                    "    df_month_info = spark.sql(sql_query)\n",
                    "    df_month_info.createOrReplaceTempView(\"df_month_info\")\n",
                    "    #df_month_info.show()\n",
                    "\n",
                    "    # Create main occupancy df with required calculations\n",
                    "\n",
                    "    sql_query = \"\"\"\n",
                    "        SELECT\n",
                    "            STG.a_service_id                                AS service_id,    \n",
                    "            S.description                                   AS service,\n",
                    "            YEAR(D.date)                                    AS year,\n",
                    "            M.month_name                                    AS month,\n",
                    "            SUM(CASE \n",
                    "                    WHEN STG.z_start_date IS NOT NULL THEN 1 \n",
                    "                    ELSE 0 \n",
                    "                END)                                        AS beddays,\n",
                    "            Z.Beds                                          AS beds,\n",
                    "            M.days_in_month,\n",
                    "\n",
                    "            --- Occupancy Calculation ---\n",
                    "            CASE\n",
                    "                WHEN M.days_in_month = 28 AND D.leap_year = 'YES'\n",
                    "                --   THEN CAST((((COUNT(1)) / NULLIF((M.days_in_month + 1) * CAST(NULLIF(Z.beds, 0) AS FLOAT), 0)) * 100) AS DECIMAL(4,3))\n",
                    "                --   ELSE CAST((((COUNT(1)) / NULLIF(M.days_in_month * CAST(NULLIF(Z.beds, 0) AS FLOAT), 0)) * 100) AS DECIMAL(4,3))\n",
                    "                THEN CAST((((COUNT(1)) / ((M.days_in_month + 1) * CAST(Z.beds AS FLOAT))) * 100) AS DECIMAL(4,3))\n",
                    "                ELSE CAST((((COUNT(1)) / (M.days_in_month * CAST(Z.beds AS FLOAT))) * 100) AS DECIMAL(4,3))\n",
                    "            END AS occupancy,\n",
                    "            \n",
                    "            --- Calculate cumulative financial year (FY) data (Jul to Jun) ----\n",
                    "\n",
                    "            SUM(COUNT(1)) OVER (PARTITION BY S.description, MIN(YEAR(add_months(D.date, 6))) ORDER BY DATE_TRUNC('month', D.date))    AS fytd_beddays,\n",
                    "            CAST(AVG(Z.beds) OVER (PARTITION BY S.description, YEAR(D.date), MONTH(D.date)) AS DECIMAL(4, 1))                         AS fytd_beds,\n",
                    "\n",
                    "            CASE\n",
                    "                WHEN M.month_number IN (2, 3, 4, 5, 6) AND D.leap_year = 'YES'\n",
                    "                THEN CAST((((SUM(COUNT(1)) OVER (PARTITION BY S.description, YEAR(ADD_MONTHS(D.date, 6)) ORDER BY DATE_TRUNC('month', D.date))) \n",
                    "                            / ((M.days_fytd + 1) * CAST(AVG(Z.beds) OVER (PARTITION BY S.description, YEAR(ADD_MONTHS(D.date, 6)) ORDER BY DATE_TRUNC('month', D.date)) AS FLOAT))) * 100) AS DECIMAL(4,1))\n",
                    "                ELSE CAST((((SUM(COUNT(1)) OVER (PARTITION BY S.description, YEAR(ADD_MONTHS(D.date, 6)) ORDER BY DATE_TRUNC('month', D.date))) \n",
                    "                            / (M.days_fytd * CAST(AVG(Z.beds) OVER (PARTITION BY S.description, YEAR(ADD_MONTHS(D.date, 6)) ORDER BY DATE_TRUNC('month', D.date)) AS FLOAT))) * 100) AS DECIMAL(4,1))\n",
                    "            END                                                                                                                             AS fytd_occupancy,\n",
                    "\n",
                    "            --- Calculate cumulative calendar year (CY) data (Jan to Dec) ---\n",
                    "\n",
                    "            SUM(COUNT(1)) OVER (PARTITION BY S.description, MIN(YEAR(add_months(D.date, 12))) ORDER BY DATE_TRUNC('month', D.date))   AS cytd_beddays,\n",
                    "            CAST(AVG(Z.beds) OVER (PARTITION BY S.description, YEAR(D.date), MONTH(D.date)) AS DECIMAL(4, 1))                         AS cytd_beds,\n",
                    "\n",
                    "            CASE\n",
                    "                WHEN M.month_number <> 1 AND D.leap_year = 'YES'\n",
                    "                THEN CAST((((SUM(COUNT(1)) OVER (PARTITION BY S.description, YEAR(ADD_MONTHS(D.date, 12)) ORDER BY DATE_TRUNC('month', D.date))) \n",
                    "                            / ((M.days_cytd + 1) * CAST(AVG(Z.beds) OVER (PARTITION BY S.description, YEAR(ADD_MONTHS(D.date, 12)) ORDER BY DATE_TRUNC('month', D.date)) AS FLOAT))) * 100) AS DECIMAL(4,1))\n",
                    "                ELSE CAST((((SUM(COUNT(1)) OVER (PARTITION BY S.description, YEAR(ADD_MONTHS(D.date, 12)) ORDER BY DATE_TRUNC('month', D.date))) \n",
                    "                            / (M.days_cytd * CAST(AVG(Z.beds) OVER (PARTITION BY S.description, YEAR(ADD_MONTHS(D.date, 12)) ORDER BY DATE_TRUNC('month', D.date)) AS FLOAT))) * 100) AS DECIMAL(4,1))\n",
                    "            END                                                                                                                          AS cytd_occupancy\n",
                    "        FROM common.dim_date AS D\n",
                    "            INNER JOIN c2_rmhs.\"\"\" + validation_table + \"\"\" AS STG ON D.date BETWEEN STG.z_start_date AND COALESCE(STG.an_end_date, CURRENT_DATE())\n",
                    "            LEFT OUTER JOIN c2_rmhs.dim_service AS S ON S.ref = STG.a_service_id\n",
                    "            LEFT OUTER JOIN df_month_info AS M ON M.month_number = MONTH(D.date)        \n",
                    "            LEFT JOIN df_avg_beds AS Z ON Z.service_id = STG.a_service_id AND D.date BETWEEN Z.period_start AND Z.period_end\n",
                    "        ---WHERE STG.z_start_date IS NOT NULL\n",
                    "        GROUP BY STG.a_service_id, S.description, YEAR(D.date), MONTH(D.date), M.month_name, M.month_number, Z.beds, M.days_in_month, M.days_fytd, M.days_cytd, D.leap_year, D.date\n",
                    "        ORDER BY S.description, YEAR(D.date), M.month_number\n",
                    "        \"\"\"\n",
                    "\n",
                    "    #print(sql_query)\n",
                    "\n",
                    "    # Execute the SQL query\n",
                    "    df_results = spark.sql(sql_query)\n",
                    "    df_results.createOrReplaceTempView(\"df_results\")\n",
                    "\n",
                    "    # Group, order occupancy results\n",
                    "\n",
                    "    # create a temporary view df_results_temp\n",
                    "    df_results.createOrReplaceTempView(\"df_results_temp\")\n",
                    "\n",
                    "    # Group results by specified columns using SQL query\n",
                    "    grouped_occupancy_results = spark.sql(\"\"\"\n",
                    "        SELECT\n",
                    "            service_id                                  AS a_service_id,\n",
                    "            service,\n",
                    "            CAST(year AS INTEGER)                       AS year,\n",
                    "            CAST(month AS STRING)                       AS month,\n",
                    "            CAST(SUM(beddays) AS INTEGER)               AS beddays,\n",
                    "            CAST(beds AS DECIMAL(4,1))                  AS beds,\n",
                    "            CAST(days_in_month AS INTEGER)              AS days_in_month,\n",
                    "            CAST(SUM(occupancy) AS DECIMAL(4,1))        AS occupancy,\n",
                    "            CAST(fytd_beddays AS INTEGER)               AS fin_year_beddays,\n",
                    "            CAST(fytd_beds AS DECIMAL(4,1))             AS fin_year_beds,\n",
                    "            CAST(fytd_occupancy AS DECIMAL(4,1))        AS fin_year_occupancy,\n",
                    "            CAST(cytd_beddays AS INTEGER)               AS cal_year_beddays,\n",
                    "            CAST(cytd_beds AS DECIMAL(4,1))             AS cal_year_beds,\n",
                    "            CAST(cytd_occupancy AS DECIMAL(4,1))        AS cal_year_occupancy\n",
                    "        FROM df_results_temp\n",
                    "        GROUP BY\n",
                    "            a_service_id,\n",
                    "            service,\n",
                    "            year,\n",
                    "            month,\n",
                    "            beds,\n",
                    "            days_in_month,\n",
                    "            fytd_beddays,\n",
                    "            fytd_beds,\n",
                    "            fytd_occupancy,\n",
                    "            cytd_beddays,\n",
                    "            cytd_beds,\n",
                    "            cytd_occupancy\n",
                    "    \"\"\")\n",
                    "\n",
                    "    # Add UID starting from 1\n",
                    "    window_spec = Window.orderBy(\"service\", \"year\", \n",
                    "                                expr(\"\"\"\n",
                    "                                CASE \n",
                    "                                    WHEN month = 'January' THEN 1\n",
                    "                                    WHEN month = 'February' THEN 2\n",
                    "                                    WHEN month = 'March' THEN 3\n",
                    "                                    WHEN month = 'April' THEN 4\n",
                    "                                    WHEN month = 'May' THEN 5\n",
                    "                                    WHEN month = 'June' THEN 6\n",
                    "                                    WHEN month = 'July' THEN 7\n",
                    "                                    WHEN month = 'August' THEN 8\n",
                    "                                    WHEN month = 'September' THEN 9\n",
                    "                                    WHEN month = 'October' THEN 10\n",
                    "                                    WHEN month = 'November' THEN 11\n",
                    "                                    WHEN month = 'December' THEN 12\n",
                    "                                END\n",
                    "                                \"\"\"))\n",
                    "    grouped_occupancy_results = grouped_occupancy_results.withColumn(\"uid\", row_number().over(window_spec))\n",
                    "\n",
                    "    # Reorder columns to make UID first\n",
                    "    columns = [\"uid\", \"a_service_id\", \"service\", \"year\", \"month\", \"beddays\", \"beds\", \"days_in_month\", \"occupancy\", \"fin_year_beddays\", \n",
                    "            \"fin_year_beds\", \"fin_year_occupancy\", \"cal_year_beddays\", \"cal_year_beds\", \"cal_year_occupancy\"]\n",
                    "    grouped_occupancy_results = grouped_occupancy_results.select(*columns)\n",
                    "\n",
                    "    # Create a view to be used in validation rules (.json)\n",
                    "    grouped_occupancy_results.createOrReplaceTempView(\"c2_rmhs_vw_stg_occupancy\")\n",
                    "\n",
                    "    # Show the final results\n",
                    "    #grouped_occupancy_results.show(10)"
                ],
                "execution_count": 90
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Save Occupancy data"
                ]
            },
            {
                "cell_type": "code",
                "source": [
                    "try:\n",
                    "    if entity_name == \"rmhs\":\n",
                    "\n",
                    "        # Remove existing Delta files - versioning is not needed\n",
                    "        if (DeltaTable.isDeltaTable(spark, occupancy_delta_path)):\n",
                    "\n",
                    "            # Convert the path to a regular URI format\n",
                    "            uri = occupancy_delta_path.replace(\"abfss:/\", \"abfs:/\")\n",
                    "\n",
                    "            # Delete the folder using Hadoop FileSystem API\n",
                    "            os.system(f'hadoop fs -rm -r \"{uri}\" > NUL 2>&1')   \n",
                    "\n",
                    "        # Write to location in delta format\n",
                    "        grouped_occupancy_results.write.format(\"delta\").mode(\"overwrite\").save(occupancy_delta_path + \"/\")\n",
                    "\n",
                    "        # Count the number of rows in the DataFrame\n",
                    "        print(\"Number of occupancy rows saved:\", grouped_occupancy_results.count())\n",
                    "\n",
                    "except Exception as e:\n",
                    "    error_msg = f\"Fact occupancy save error: {e}\"\n",
                    "    print(f\"Error: {e}\")       "
                ],
                "execution_count": 91
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Perform row validation"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "# Create views used in the validation rules\n",
                    "if entity_name == \"rmhs\":\n",
                    "\n",
                    "    # vw_dim_diagnosis\n",
                    "    df_diagnosis = spark.sql(f\"SELECT uid, ref, description, is_current FROM common.dim_diagnosis WHERE uid BETWEEN 1 AND 1191\")\n",
                    "    df_diagnosis.createOrReplaceTempView(\"c2_rmhs_vw_dim_diagnosis\")\n",
                    "\n",
                    "    # dim_marital_status\n",
                    "    df_marital_status = spark.sql(f\"SELECT ROW_NUMBER() OVER (PARTITION BY D.uid ORDER BY D.ref) AS uid, D.ref, D.description, D.is_current \\\n",
                    "        FROM (SELECT 1 AS uid, ref, description, is_current FROM common.dim_marital_status WHERE mhc_collection_uid in (SELECT uid FROM common.dim_mhc_collection WHERE ref = 'c2')) D\")\n",
                    "    df_marital_status.createOrReplaceTempView(\"c2_rmhs_vw_dim_marital_status\")\n",
                    "\n",
                    "    # vw_dim_area_of_usual_residence\n",
                    "    f_area_of_usual_residence = spark.sql(f\"SELECT uid, postcode AS ref, locality AS description, is_current FROM common.fact_locality\")\n",
                    "    f_area_of_usual_residence.createOrReplaceTempView(\"c2_rmhs_vw_dim_area_of_usual_residence\")\n",
                    "\n",
                    "    # vw_dim_exit1\n",
                    "    df_exit1 = spark.sql(f\"SELECT ROW_NUMBER() OVER (PARTITION BY D.uid ORDER BY D.ref) AS uid, D.ref, D.description, D.is_current \\\n",
                    "        FROM (SELECT 1 AS uid, ref, description, is_current FROM common.dim_evaluation WHERE evaluation_type = 'rmhs_x1') D\")\n",
                    "    df_exit1.createOrReplaceTempView(\"c2_rmhs_vw_dim_exit1\")\n",
                    "\n",
                    "    # vw_dim_exit2\n",
                    "    df_exit2 = spark.sql(f\"SELECT ROW_NUMBER() OVER (PARTITION BY D.uid ORDER BY D.ref) AS uid, D.ref, D.description, D.is_current \\\n",
                    "        FROM (SELECT 1 AS uid, ref, description, is_current FROM common.dim_evaluation WHERE evaluation_type = 'rmhs_x2') D\")\n",
                    "    df_exit2.createOrReplaceTempView(\"c2_rmhs_vw_dim_exit2\")\n",
                    "\n",
                    "elif entity_name == \"poc\":\n",
                    "    \n",
                    "    df_vw_stg_validation_poc = spark.sql(\"\"\"\n",
                    "    SELECT \n",
                    "\t    STG.a_service_id, STG.b_person_id, STG.c_episode_id, STG.d_poc_id, STG.e_poc_type, STG.f_poc_start_date, STG.g_poc_end_date, STG.h_poc_leave_days, \n",
                    "\t    STG.specification, FE.z_start_date, FE.an_end_date, FE.ae_number_of_leave_days\n",
                    "    FROM c2_rmhs.\"\"\" + validation_table + \"\"\" STG\n",
                    "        LEFT OUTER JOIN c2_rmhs.dim_service A ON A.ref = STG.a_service_id\n",
                    "        LEFT OUTER JOIN c2_rmhs.dim_phase_of_care_type E ON E.ref = STG.e_poc_type\n",
                    "        LEFT OUTER JOIN c2_rmhs.fact_episode FE ON FE.a_service_uid = A.uid AND FE.k_person_id = STG.b_person_id AND FE.y_episode_id = STG.c_episode_id\n",
                    "        LEFT OUTER JOIN c2_rmhs.dim_specification SPEC ON SPEC.ref = STG.specification\n",
                    "    \"\"\")\n",
                    "    df_vw_stg_validation_poc.createOrReplaceTempView(\"c2_rmhs_vw_stg_validation_poc\")\n",
                    "\n",
                    "else: \n",
                    "    print(\"Unknown input file type: \" + entity_name) \n",
                    "    raise ValueError(\"Unknown input file type. Please investigate\")\n",
                    "\n",
                    "# Counter for bad sql statements, run generate system error if > 0\n",
                    "error_counter = 0\n",
                    "\n",
                    "# For each new incoming UID\n",
                    "for index, row in rulespd_df.iterrows():\n",
                    "    \n",
                    "    # Create SQL expression from validation rule elements\n",
                    "    if entity_name == \"rmhs\":\n",
                    "\n",
                    "        sql_expression = f\"\"\"\n",
                    "            SELECT \n",
                    "                '{file_app_id}'                 AS app_id\n",
                    "                ,'{file_timestamp}'             AS datetime_stamp\n",
                    "                ,'rmhs'                         AS collection_name\n",
                    "                ,'person episodes'              AS collection_description\n",
                    "                ,'{row['edit']}'                AS edit\n",
                    "                ,{row['uid']}            \n",
                    "                ,'{row['validation_check']}'    AS validation_check\n",
                    "                ,'{row['data_element']}'        AS data_element\n",
                    "                ,{row['data_element_value']}    AS data_element_value\n",
                    "                ,'{row['severity']}'            AS severity           \n",
                    "                ,'{row['check_id']}'            AS check_id            \n",
                    "            FROM {row['from']}\n",
                    "            WHERE {row['where']}\n",
                    "            \"\"\"   \n",
                    "\n",
                    "    elif entity_name == \"poc\":      \n",
                    "\n",
                    "        sql_expression = f\"\"\"\n",
                    "            SELECT \n",
                    "                '{file_app_id}'                 AS app_id\n",
                    "                ,'{file_timestamp}'             AS datetime_stamp\n",
                    "                ,'rmhs'                         AS collection_name\n",
                    "                ,'phase of care'                AS collection_description\n",
                    "                ,'{row['edit']}'                AS edit\n",
                    "                ,{row['uid']}            \n",
                    "                ,'{row['validation_check']}'    AS validation_check\n",
                    "                ,'{row['data_element']}'        AS data_element\n",
                    "                ,{row['data_element_value']}    AS data_element_value\n",
                    "                ,'{row['severity']}'            AS severity           \n",
                    "                ,'{row['check_id']}'            AS check_id            \n",
                    "            FROM {row['from']}\n",
                    "            WHERE {row['where']}\n",
                    "            \"\"\" \n",
                    "\n",
                    "    table_with_timestamp = f\"c2_rmhs.{validation_table}\"\n",
                    "    sql_expression = sql_expression.replace(\"c2_rmhs.stg_validation_rmhs\", table_with_timestamp).replace(\"c2_rmhs.stg_validation_poc\", table_with_timestamp)\n",
                    "\n",
                    "    try:\n",
                    "        # Run the SQL expression and return result\n",
                    "        result = spark.sql(sql_expression)\n",
                    "    \n",
                    "    except Exception as e:\n",
                    "        # Add a System Error if the validation rule was not valid\n",
                    "        print(f\"Check {row['check_id']} ({row['validation_check']}) failed to run.\")\n",
                    "        print(f\"Error: {e}\")\n",
                    "        print(sql_expression)\n",
                    "        error_counter += 1\n",
                    "        c_check_id = f\"{row['check_id']}\"\n",
                    "\n",
                    "        if entity_name == \"rmhs\":\n",
                    "\n",
                    "            new_exception = Row( \\\n",
                    "                app_id = file_app_id, \\\n",
                    "                datetime_stamp = file_timestamp, \\\n",
                    "                collection_name = \"rmhs\", \\\n",
                    "                collection_description = \"person episodes\", \\\n",
                    "                edit = \"\", \\\n",
                    "                a_service_id = \"\", \\\n",
                    "                b_date_of_referral = \"\", \\\n",
                    "                k_person_id = \"\", \\\n",
                    "                validation_check = \"Validation rule failed to run\", \\\n",
                    "                data_element = \"\", \\\n",
                    "                data_element_value = \"\", \\\n",
                    "                severity=\"System\", \\\n",
                    "                check_id = c_check_id)        \n",
                    "\n",
                    "        elif entity_name == \"poc\":\n",
                    "\n",
                    "            new_exception = Row( \\\n",
                    "                app_id = file_app_id, \\\n",
                    "                datetime_stamp = file_timestamp, \\\n",
                    "                collection_name = \"rmhs\", \\\n",
                    "                collection_description = \"phase of care\", \\\n",
                    "                edit = \"\", \\\n",
                    "                a_service_id = \"\", \\\n",
                    "                b_person_id = \"\", \\\n",
                    "                c_episode_id = \"\", \\\n",
                    "                d_poc_id = \"\", \\\n",
                    "                validation_check = \"Validation rule failed to run\", \\\n",
                    "                data_element = \"\", \\\n",
                    "                data_element_value = \"\", \\\n",
                    "                severity=\"System\", \\\n",
                    "                check_id = c_check_id)\n",
                    "\n",
                    "        # Convert the new row to a DataFrame\n",
                    "        new_exception_df = spark.createDataFrame([new_exception])\n",
                    "\n",
                    "        # Append the new row to the existing DataFrame\n",
                    "        exception_df = exception_df.union(new_exception_df)\n",
                    "    \n",
                    "    # If the query returns a result, append it to exception_df\n",
                    "    \n",
                    "    try: result\n",
                    "\n",
                    "    except NameError:\n",
                    "        print(\"The first check failed\")\n",
                    "\n",
                    "    else:\n",
                    "        if result.count() > 0:\n",
                    "            \n",
                    "            # Add new error to existing errors\n",
                    "            exception_df = exception_df.union(result)\n",
                    "\n",
                    "critical_errors = exception_df.filter(exception_df['severity'] == 'Critical').count()\n",
                    "print(\"Critical errors: \" + str(critical_errors))\n",
                    "\n",
                    "warnings = exception_df.filter(exception_df['severity'] == 'Warning').count()\n",
                    "print(\"Warnings: \" + str(warnings))\n",
                    "\n",
                    "print(\"System errors: \" + str(error_counter))\n",
                    "\n",
                    "# Remove validation table\n",
                    "spark.sql(f\"DROP TABLE IF EXISTS {target_database}.c2_rmhs\")\n",
                    "\n",
                    "#print(\"validation_table: \" + validation_table)\n",
                    "spark.sql(f\"DROP TABLE IF EXISTS {target_database}.{validation_table}\")"
                ],
                "execution_count": 92
            },
            {
                "cell_type": "markdown",
                "metadata": {
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "## Manage notebook activity in pipeline"
                ]
            },
            {
                "cell_type": "code",
                "metadata": {
                    "jupyter": {
                        "source_hidden": false,
                        "outputs_hidden": false
                    },
                    "nteract": {
                        "transient": {
                            "deleting": false
                        }
                    }
                },
                "source": [
                    "if exception_df.count() > 0:\n",
                    "\n",
                    "    if entity_name == \"rmhs\":\n",
                    "        exception_df = exception_df.orderBy(col(\"severity\").asc(), col(\"k_person_id\").asc(), col(\"edit\").asc())        \n",
                    "\n",
                    "    elif entity_name == \"poc\":\n",
                    "        exception_df = exception_df.orderBy(col(\"severity\").asc(), col(\"b_person_id\").asc(), col(\"edit\").asc())\n",
                    "\n",
                    "# Always save the result of the validation as this will be sent back to the DSP\n",
                    "exception_df.toPandas().to_csv(exception_file_path, header=True, index=False)\n",
                    "print(\"Total number of exceptions: \" + str(exception_df.count()))\n",
                    "\n",
                    "if error_counter > 0 or critical_errors > 0:\n",
                    "    raise ValueError(\"Submission failed, at least one system or critical error was found - please see validation report for more info\")    \n",
                    "else: \n",
                    "    print(\"Row validation was successful, data will be merged\")"
                ],
                "execution_count": 93
            }
        ]
    }
}